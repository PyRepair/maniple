```python
def call(self, inputs, states, training=None):
    # dropout matrices for input units
    dp_mask = self._dropout_mask
    # dropout matrices for recurrent units
    rec_dp_mask = self._recurrent_dropout_mask

    h_tm1 = states[0]  # previous memory state
    c_tm1 = states[1]  # previous carry state

    if self.implementation == 1:
        if 0 < self.dropout < 1.:
            inputs_i = inputs * dp_mask[0]
            inputs_f = inputs * dp_mask[1]
            inputs_c = inputs * dp_mask[2]
            inputs_o = inputs * dp_mask[3]
        else:
            inputs_i = inputs
            inputs_f = inputs
            inputs_c = inputs
            inputs_o = inputs
        x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i
        x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f
        x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c
        x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o

        if 0 < self.recurrent_dropout < 1.:
            h_tm1_i = h_tm1 * rec_dp_mask[0]
            h_tm1_f = h_tm1 * rec_dp_mask[1]
            h_tm1_c = h_tm1 * rec_dp_mask[2]
            h_tm1_o = h_tm1 * rec_dp_mask[3]
        else:
            h_tm1_i = h_tm1
            h_tm1_f = h_tm1
            h_tm1_c = h_tm1
            h_tm1_o = h_tm1
        i = self.recurrent_activation(x_i + K.dot(h_tm1_i, self.recurrent_kernel_i))
        f = self.recurrent_activation(x_f + K.dot(h_tm1_f, self.recurrent_kernel_f))
        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c, self.recurrent_kernel_c))
        o = self.recurrent_activation(x_o + K.dot(h_tm1_o, self.recurrent_kernel_o))
    else:
        if 0. < self.dropout < 1.:
            inputs *= dp_mask[0]
        z = K.dot(inputs, self.kernel)
        if 0. < self.recurrent_dropout < 1.:
            h_tm1 *= rec_dp_mask[0]
        z += K.dot(h_tm1, self.recurrent_kernel)
        if self.use_bias:
            z = K.bias_add(z, self.bias)

        z0 = z[:, :self.units]
        z1 = z[:, self.units: 2 * self.units]
        z2 = z[:, 2 * self.units: 3 * self.units]
        z3 = z[:, 3 * self.units:]

        i = self.recurrent_activation(z0)
        f = self.recurrent_activation(z1)
        c = f * c_tm1 + i * self.activation(z2)
        o = self.recurrent_activation(z3)

    h = o * self.activation(c)
    if 0 < self.recurrent_dropout:
        if training is not None:
            h._uses_learning_phase = True
    return h, [h, c]
```